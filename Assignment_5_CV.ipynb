{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How can each of these parameters be fine-tuned? \n",
    "• Number of hidden layers \n",
    "• Network architecture (network depth)\n",
    "\n",
    "• Each layer's number of neurons (layer width)\n",
    "\n",
    "• Form of activation\n",
    "\n",
    "• Optimization and learning\n",
    "\n",
    "• Learning rate and decay schedule\n",
    "\n",
    "• Mini batch size\n",
    "\n",
    "• The number of epochs (and early stopping criteria)\n",
    "\n",
    "• Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "• L2 normalization\n",
    "\n",
    "• Drop out layers\n",
    "• Data augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Number of hidden layers - In this case, the number of neurons in every layer is set to be the same. It also can be made different. The number of neurons should be adjusted to the solution complexity. The task with a more complex level to predict needs more neurons. The number of neurons range is set to be from 10 to 100.\n",
    "\n",
    "Learning rate - We will also tune the learning rate. Learning rate controls the step size for a model to reach the minimum loss function. A higher learning rate makes the model learn faster, but it may miss the minimum loss function and only reach the surrounding of it. A lower learning rate gives a better chance to find a minimum loss function. As a tradeoff lower learning rate needs higher epochs, or more time and memory capacity resources.\n",
    "\n",
    "Mini batch size - we can assign batch size so that not all of the training data are given to the model at the same time. Batch size is the number of training data sub-samples for the input. If the training dataset has 77,500 observations and the batch size is 1000, the model will learn 77 times with 1000 training data sub-samples and another last learning from the 500 training data sub-samples. The smaller batch size makes the learning process faster, but the variance of the validation dataset accuracy is higher. A bigger batch size has a slower learning process, but the validation dataset accuracy has a lower variance.\n",
    "\n",
    "Optimization and learning - The optimizer is responsible to change the learning rate and weights of neurons in the neural network to reach the minimum loss function. Optimizer is very important to achieve the possible highest accuracy or minimum loss. There are 7 optimizers to choose from. Each has a different concept behind it.\n",
    "\n",
    "Optimization and learning - An activation function is a parameter in each layer. Input data are fed to the input layer, followed by hidden layers, and the final output layer. The output layer contains the output value. The input values moving from a layer to another layer keep changing according to the activation function. The activation function decides how to compute the input values of a layer into output values. The output values of a layer are then passed to the next layer as input values again. The next layer then computes the values into output values for another layer again. There are 9 activation functions to tune in to this demonstration. Each activation function has its own formula (and graph) to compute the input values.\n",
    "\n",
    "The number of epochs - The number of times a whole dataset is passed through the neural network model is called an epoch. One epoch means that the training dataset is passed forward and backward through the neural network once. A too-small number of epochs results in underfitting because the neural network has not learned much enough. The training dataset needs to pass multiple times or multiple epochs are required. On the other hand, too many epochs will lead to overfitting where the model can predict the data very well, but cannot predict new unseen data well enough. The number of epoch must be tuned to gain the optimal result. This demonstration searches for a suitable number of epochs between 20 to 100.\n",
    "\n",
    "Each layer's number of neurons - Layers in Neural Network also determine the result of the prediction model. A smaller number of layers is enough for a simpler problem, but a larger number of layers is needed to build a model for a more complicated problem. The number of layers can be tuned using the “for loop” iteration. This demonstration tune the number of layers two times. Each time, the number of layers is tuned between 1 to 3.\n",
    "\n",
    "Overfitting that be avoided by using regularization techniques. - Inserting regularization layers in a neural network can help prevent overfitting. This demonstration tries to tune whether to add regularization layers or not. There are two regularization layers to use here.Batch normalization is placed after the first hidden layers. The batch normalization layer normalizes the values passed to it for every batch. This is similar to standard scaler in conventional Machine Learning.\n",
    "\n",
    "Drop out layers - After batch normalization another regularization layer is the Dropout layer. The dropout layer, as its name suggests, randomly drops a certain number of neurons in a layer. The dropped neurons are not used anymore. The rate of how much percentage of neurons to drop is set in the dropout rate. \n",
    "\n",
    " L2 normalization - L2 regularization, also known as Ridge Regression, adds a penalty term proportional to the square of the model's parameters. This encourages the model to use all of the parameters but to reduce their values, resulting in a model that is less complex and less prone to overfitting\n",
    " \n",
    "Data augmentation - Data augmentation is the approach of generating more training data from the existing training samples by the samples through a number of random augmenting transformations that yield believable looking data(images in image dataset). The goal is that at the training time, the model will never see the exact same data or feature(picture) twice. This helps expose the model to more aspects of the data and generalize better on the data as it proves better for the neural network. Also, given infinite data, the model would be exposed to every possible aspect of data distribution at hand so, the model will never overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
